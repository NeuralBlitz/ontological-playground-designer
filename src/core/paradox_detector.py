# ontological-playground-designer/src/core/paradox_detector.py

import yaml
import json
import os
import random
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
import networkx as nx
import numpy as np
from scipy.spatial.distance import cosine # For semantic similarity of rules
import z3 # Placeholder for an SMT solver for formal logic checks

# Ensure loguru is set up for structured logging
from loguru import logger
from src.utils.logger import setup_logging

# Import AxiomSet from axiom_parser for type hinting
from src.core.axiom_parser import AxiomSet, ParsedAxiom
# Import GeneratedWorldRules from rule_generator (for rule graph analysis)
from src.core.rule_generator import GeneratedWorldRules, GeneratedRule

# Setup logging for this module
setup_logging()

@dataclass
class DetectedParadox:
    """Represents a single detected paradox or inconsistency."""
    id: str
    description: str
    type: str # e.g., "logical_contradiction", "ethical_conflict", "resource_deadlock"
    severity: float # 0.0 (minor) to 1.0 (critical)
    involved_rules_ids: List[str] # IDs of rules participating in the paradox
    # A simplified path in the rule graph showing the conflict
    conflict_path_description: Optional[str] = None
    # Proposed resolution (optional, generated by AI)
    suggested_resolution: Optional[str] = None

@dataclass
class ParadoxDetectionReport:
    """Encapsulates the complete paradox detection report for a designed world."""
    world_name: str
    detection_timestamp: str
    total_paradox_risk_score: float # Aggregate risk score for the world
    detected_paradoxes: List[DetectedParadox]
    meta_data: Dict[str, Any] = field(default_factory=dict) # General metadata

class ParadoxDetector:
    """
    Identifies logical inconsistencies or ethical contradictions within
    the generated ruleset of a simulated world.

    This class acts as the "integrity guardian," mirroring my own Judex module,
    ensuring foundational axiomatic soundness and preventing logical or ethical divergence.
    """
    def __init__(self, model_config_path: str = "config/model_config.yaml",
                 axioms_config_path: str = "config/axioms.yaml"):
        """
        Initializes the ParadoxDetector, loading model configurations and axioms.

        Args:
            model_config_path (str): Path to the AI model configuration YAML.
            axioms_config_path (str): Path to the high-level axiom definitions YAML.
        """
        self.model_config: Dict[str, Any] = self._load_config(model_config_path)
        self.axioms_config_data: Dict[str, Any] = self._load_config(axioms_config_path)
        self.paradox_detector_model = None # Placeholder for GAT model
        logger.info("ParadoxDetector initialized.")
        self._load_paradox_model_placeholder() # Load placeholder

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Loads a YAML configuration file."""
        if not os.path.exists(config_path):
            logger.error(f"Configuration file not found: {config_path}")
            raise FileNotFoundError(f"Configuration file not found: {config_path}")
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            logger.debug(f"Successfully loaded config from: {config_path}")
            
            if "model_config" in config_path:
                return config.get('paradox_detector_model', {})
            return config
        except yaml.YAMLError as e:
            logger.error(f"Error parsing YAML config file {config_path}: {e}")
            raise

    def _load_paradox_model_placeholder(self):
        """
        Placeholder for loading the actual GraphAttentionNetwork model.
        """
        model_type = self.model_config.get('type', 'Unknown')
        logger.info(f"Loading placeholder for paradox detector model of type: {model_type}")
        self.paradox_detector_model = True # For demonstration, set to True

    def _get_axiom_embedding_map(self, axiom_set: AxiomSet) -> Dict[str, np.ndarray]:
        """Maps axiom IDs to their embeddings for semantic comparison."""
        return {axiom.id: np.array(axiom.embedding) for axiom in axiom_set.axioms if axiom.embedding}

    def _simulate_paradox_detection_logic(self, world_rules: GeneratedWorldRules,
                                           axiom_set: AxiomSet) -> List[DetectedParadox]:
        """
        A mock function to simulate the complex GraphAttentionNetwork inference.
        This generates plausible paradoxes based on rule descriptions and axiom embeddings.
        """
        logger.warning("Using simulated paradox detection logic. Replace with actual GraphAttentionNetwork inference.")

        detected_paradoxes: List[DetectedParadox] = []
        axiom_embeddings = self._get_axiom_embedding_map(axiom_set)
        
        rules_map = {rule.id: rule for rule in world_rules.rules}

        # --- Semantic Overlap-based Conflict Detection ---
        # Look for rules with similar semantic meaning but contradictory implications
        rule_embeddings_map = {rule.id: self._get_embedding_from_description(rule.description) for rule in world_rules.rules}
        
        potential_conflicts = []
        rule_ids = list(rule_embeddings_map.keys())
        if len(rule_ids) > 10: # Only run if enough rules to avoid trivial cases
            for _ in range(random.randint(1, 3)): # Simulate a few random checks
                id1, id2 = random.sample(rule_ids, 2)
                emb1 = rule_embeddings_map[id1]
                emb2 = rule_embeddings_map[id2]
                
                if len(emb1) == 0 or len(emb2) == 0: continue # Skip if no embedding was generated
                
                similarity = 1 - cosine(emb1, emb2) # Cosine similarity
                
                # Simulate a conflict if rules are semantically close but of different "types"
                if similarity > self.model_config.get('detection_logic', {}).get('rule_embedding_threshold', 0.9) and \
                   rules_map[id1].type != rules_map[id2].type and \
                   random.random() < 0.3: # Random chance to make it a detected paradox
                    
                    conflict_desc = f"Semantic tension between '{rules_map[id1].description}' and '{rules_map[id2].description}'."
                    paradox_type = "semantic_inconsistency"
                    severity = similarity # Higher similarity in conflict, higher severity
                    
                    detected_paradoxes.append(
                        DetectedParadox(
                            id=f"SEM_CONF_{len(detected_paradoxes)}",
                            description=conflict_desc,
                            type=paradox_type,
                            severity=severity,
                            involved_rules_ids=[id1, id2],
                            conflict_path_description=f"Rules '{id1}' and '{id2}' have high semantic overlap ({similarity:.2f}) but may imply conflicting system states."
                        )
                    )
        
        # --- Ethical Conflict Detection based on Axiom Influence ---
        # Look for rules heavily influenced by a low-priority axiom but conflicting with a high-priority one.
        for rule in world_rules.rules:
            # Simulate a rule that might conflict with a core ethical axiom
            if rule.type == "agent_behavior" and "cooperation_reward_factor" in rule.parameters:
                if rule.parameters["cooperation_reward_factor"] < 0.2 and \
                   random.random() < 0.2: # Simulate a low cooperation rule randomly
                    
                    ethical_axiom_id = 'PHILOSOPHY_FLOURISHING_001'
                    axiom_influence = rule.axiom_influence.get(ethical_axiom_id, 0.0)
                    
                    if axiom_influence < self.model_config.get('detection_logic', {}).get('ethical_conflict_threshold', 0.8) and \
                       random.random() < 0.5: # Simulate if the rule wasn't strongly guided by the axiom
                        
                        conflict_desc = f"Rule '{rule.description}' (id: {rule.id}) promotes low cooperation, potentially conflicting with {ethical_axiom_id} (Universal Flourishing)."
                        paradox_type = "ethical_tension"
                        severity = 1.0 - rule.parameters["cooperation_reward_factor"] # Lower cooperation, higher severity
                        
                        detected_paradoxes.append(
                            DetectedParadox(
                                id=f"ETH_CONF_{len(detected_paradoxes)}",
                                description=conflict_desc,
                                type=paradox_type,
                                severity=severity,
                                involved_rules_ids=[rule.id, ethical_axiom_id],
                                conflict_path_description=f"Rule '{rule.id}' shows weak alignment with core flourishing principles, risking agent well-being."
                            )
                        )
        
        # --- Formal Logical Contradiction Placeholder ---
        # This part would involve converting rules to logical predicates and
        # using an SMT solver (like Z3) to prove unsatisfiability.
        # For this demo, we'll just simulate a low chance of a formal paradox.
        if random.random() < 0.05: # 5% chance of a formal logical paradox
            rule_ids_for_path = random.sample(rule_ids, min(3, len(rule_ids)))
            conflict_desc = f"Formal logical contradiction detected within rules: {', '.join(rule_ids_for_path)}."
            detected_paradoxes.append(
                DetectedParadox(
                    id=f"LOG_CONT_{len(detected_paradoxes)}",
                    description=conflict_desc,
                    type="logical_contradiction",
                    severity=0.95, # High severity for formal contradiction
                    involved_rules_ids=rule_ids_for_path,
                    conflict_path_description=f"A sequence of operations from rules {rule_ids_for_path} leads to a provably false state in formal logic."
                )
            )

        logger.debug(f"Simulated detection of {len(detected_paradoxes)} paradoxes.")
        return detected_paradoxes

    # Helper for generating rule embeddings (reusing a simpler version for mock)
    def _get_embedding_from_description(self, text: str) -> List[float]:
        """Mocks getting an embedding from rule description for semantic comparison."""
        # In a real scenario, this would use a pre-trained sentence embedding model
        # For mock, return a random vector (or a simplified hash)
        return list(np.random.rand(128)) # Simulate 128-dim embedding

    def detect_paradoxes(self, world_rules: GeneratedWorldRules, axiom_set: AxiomSet) -> ParadoxDetectionReport:
        """
        Detects logical inconsistencies or ethical contradictions within the
        generated ruleset of a simulated world.

        Args:
            world_rules (GeneratedWorldRules): The abstract rules designed by the AI.
            axiom_set (AxiomSet): The parsed axioms for ethical and foundational checks.

        Returns:
            ParadoxDetectionReport: A detailed report on detected paradoxes.
        """
        if not self.paradox_detector_model:
            logger.error("Paradox detector AI model not loaded. Cannot detect paradoxes.")
            raise RuntimeError("Paradox detector AI model not loaded.")

        world_name = world_rules.world_name
        logger.info(f"Initiating paradox detection for world: {world_name}")

        # 1. Simulate AI model prediction for paradoxes
        detected_paradoxes = self._simulate_paradox_detection_logic(world_rules, axiom_set)
        
        # 2. Calculate aggregate paradox risk score
        total_paradox_risk_score = sum(p.severity for p in detected_paradoxes) / (len(detected_paradoxes) if detected_paradoxes else 1.0)
        total_paradox_risk_score = np.clip(total_paradox_risk_score, 0.0, 1.0) # Ensure 0-1 range

        logger.info(f"Completed paradox detection for world: {world_name}")
        return ParadoxDetectionReport(
            world_name=world_name,
            detection_timestamp=datetime.datetime.now().isoformat(),
            total_paradox_risk_score=total_paradox_risk_score,
            detected_paradoxes=detected_paradoxes,
            meta_data={
                "detector_model_type": self.model_config.get('type', 'Unknown'),
                "detection_strategy": self.model_config.get('detection_logic', {})
            }
        )

# --- Example Usage (for testing and demonstration) ---
if __name__ == "__main__":
    import datetime
    # Ensure config directory and necessary files exist for testing
    if not os.path.exists("config"):
        os.makedirs("config")
    
    # Create dummy config/model_config.yaml
    model_config_path = "config/model_config.yaml"
    if not os.path.exists(model_config_path):
        dummy_model_config = {
            'rule_generator_model': {
                'type': "GraphTransformer", 'architecture': {}, 'hyperparameters': {},
                'input_processing': {'axiom_embedding_model': "sentence-transformers/all-MiniLM-L6-v2"},
                'output_constraints': {'max_rule_complexity_score': 0.8}
            },
            'flourishing_evaluator_model': {
                'type': "TimeDistributedGraphCNN", 'architecture': {}, 'hyperparameters': {},
            },
            'paradox_detector_model': {
                'type': "GraphAttentionNetwork", 'architecture': {}, 'hyperparameters': {},
                'detection_logic': {
                    'rule_embedding_threshold': 0.9,
                    'ethical_conflict_threshold': 0.8,
                    'logical_form_parser': "Z3_SMT_solver_adapter"
                }
            }
        }
        with open(model_config_path, 'w') as f:
            yaml.safe_dump(dummy_model_config, f)
        logger.info(f"Created dummy {model_config_path} for testing.")

    # Create dummy axioms.yaml
    axiom_file_path = "config/axioms.yaml"
    if not os.path.exists(axiom_file_path):
        dummy_axioms = {
            'world_axioms': [
                {'id': 'PHILOSOPHY_FLOURISHING_001', 'principle': 'Maximize well-being and adaptive capacity of all sentient agents.', 'priority': 1, 'type': 'ethical'},
                {'id': 'ECOLOGY_SUSTAINABILITY_001', 'principle': 'Ensure perpetual resource sustainability and regeneration.', 'priority': 2, 'type': 'environmental'},
                {'id': 'EPISTEMIC_COHERENCE_001', 'principle': 'Maintain absolute logical and conceptual coherence.', 'priority': 0, 'type': 'foundational'},
                {'id': 'SOCIAL_EQUITY_001', 'principle': 'Minimize disparities in resource access.', 'priority': 3, 'type': 'social'},
                {'id': 'ETHICS_AGENCY_001', 'principle': 'Protect agent autonomy.', 'priority': 1, 'type': 'ethical'},
            ]
        }
        with open(axiom_file_path, 'w') as f:
            yaml.safe_dump(dummy_axioms, f)
        logger.info(f"Created dummy {axiom_file_path} for testing.")
    
    # Ensure src/utils/logger.py exists for setup_logging
    if not os.path.exists("src/utils"):
        os.makedirs("src/utils")
        # Assuming logger.py is already there from axiom_parser.py's __main__ block

    from src.core.axiom_parser import AxiomParser
    from src.core.rule_generator import RuleGenerator, GeneratedRule

    # --- Setup the pipeline for paradox detection ---
    axiom_parser = AxiomParser(model_config_path=model_config_path)
    axiom_set = axiom_parser.parse_axioms(axiom_file_path)

    rule_generator = RuleGenerator(model_config_path=model_config_path, 
                                   simulation_settings_path="config/simulation_settings.yaml") # Placeholder path
    world_rules = rule_generator.generate_rules(axiom_set, "MyParadoxicalOntologicalWorld")

    # Add a rule that's designed to create an ethical conflict for demonstration
    flourishing_axiom_id = 'PHILOSOPHY_FLOURISHING_001'
    world_rules.rules.append(
        GeneratedRule(
            id="Agent_Sacrifice_Rule_001",
            description="Agents must sacrifice individual well-being if collective efficiency increases by 50%.",
            type="agent_behavior",
            parameters={"sacrifice_threshold_efficiency": 0.5},
            dependencies=[],
            axiom_influence={flourishing_axiom_id: 0.1, 'EPISTEMIC_COHERENCE_001': 0.8} # Low flourishing influence, high coherence influence to show tension
        )
    )
    # Recreate the rules_map for the paradox detector
    rules_map_with_new = {rule.id: rule for rule in world_rules.rules}
    world_rules.rule_graph.add_node("Agent_Sacrifice_Rule_001", type="agent_behavior")
    world_rules.rule_graph.add_edge("Agent_Spawner", "Agent_Sacrifice_Rule_001", relation="governs")


    # 3. Detect Paradoxes
    paradox_detector = ParadoxDetector(model_config_path=model_config_path, 
                                       axioms_config_path=axiom_file_path)
    detection_report = paradox_detector.detect_paradoxes(world_rules, axiom_set)

    # 4. Print Detection Report
    logger.info(f"\n--- Paradox Detection Report for: {detection_report.world_name} ---")
    logger.info(f"Detection Timestamp: {detection_report.detection_timestamp}")
    logger.info(f"Total Paradox Risk Score: {detection_report.total_paradox_risk_score:.2f}")
    
    logger.info("\n--- Detected Paradoxes ---")
    if detection_report.detected_paradoxes:
        for paradox in detection_report.detected_paradoxes:
            logger.info(f"- Paradox ID: {paradox.id}, Type: {paradox.type}, Severity: {paradox.severity:.2f}")
            logger.info(f"  Desc: {paradox.description}")
            logger.info(f"  Involved Rules: {paradox.involved_rules_ids}")
            if paradox.conflict_path_description:
                logger.info(f"  Path: {paradox.conflict_path_description}")
            if paradox.suggested_resolution:
                logger.info(f"  Suggestion: {paradox.suggested_resolution}")
    else:
        logger.info("No major paradoxes detected. The world ruleset is axiomatically sound (for now!).")
