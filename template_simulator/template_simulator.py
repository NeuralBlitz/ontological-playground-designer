# ontological-playground-designer/simulators/template_simulator/template_simulator.py

import json
import os
import random
from typing import Dict, Any, List
import datetime

# Ensure loguru is set up for structured logging
from loguru import logger
from src.utils.logger import setup_logging

# Setup logging for this module
setup_logging()

@dataclass
class AgentState:
    """Represents the state of a single agent in the simulation."""
    id: str
    species: str
    x: int
    y: int
    energy: float
    well_being: float
    cooperation_tendency: float # Inherited/modified from world rules
    is_alive: bool = True

@dataclass
class ResourceNode:
    """Represents a resource node in the simulation environment."""
    x: int
    y: int
    type: str
    amount: float
    regen_rate: float

@dataclass
class SimulationState:
    """Encapsulates the full state of the simulation at a given time step."""
    time_step: int
    agents: List[AgentState]
    resources: List[ResourceNode]
    world_metrics: Dict[str, Any]
    log_messages: List[str] = field(default_factory=list)

class TemplateSimulator:
    """
    A basic, agent-based simulator designed to run worlds generated by the
    Ontological Playground Designer.

    It interprets a simplified set of rules and parameters from a compiled
    world configuration to simulate agent behavior, resource dynamics,
    and environmental changes over time.
    """
    def __init__(self, world_config: Dict[str, Any]):
        """
        Initializes the simulator with a compiled world configuration.

        Args:
            world_config (Dict[str, Any]): The configuration of the world to simulate,
                                            as output by WorldCompiler.
        """
        self.world_config = world_config
        self.world_name = world_config.get('world_metadata', {}).get('name', 'UnnamedWorld')
        self.sim_settings = world_config.get('simulation_defaults', {})
        self.generated_rules = world_config.get('generated_world_rules', {})
        
        # --- Initialize World State ---
        self.current_time_step = 0
        self.max_simulation_steps = self.sim_settings.get('max_simulation_steps', 50000)
        self.world_size = self.sim_settings.get('initial_world_size', {'x_dim': 100, 'y_dim': 100})
        
        self.agents: List[AgentState] = []
        self.resources: List[ResourceNode] = []
        self.simulation_log: List[SimulationState] = []
        self._initialize_world_state()

        logger.info(f"TemplateSimulator initialized for world: '{self.world_name}'")
        logger.info(f"Max simulation steps: {self.max_simulation_steps}")
        logger.info(f"Initial agents: {len(self.agents)}, Resources: {len(self.resources)}")

    def _initialize_world_state(self):
        """
        Sets up the initial state of agents and resources based on world_config.
        """
        # Initialize Agents
        initial_agent_count = self.sim_settings.get('initial_agent_count', 50)
        agent_species_diversity = self.sim_settings.get('agent_species_diversity', 2)
        base_coop = self.sim_settings.get('initial_agent_cooperation_tendency', 0.5)

        for i in range(initial_agent_count):
            species = f"Species_{random.randint(1, agent_species_diversity)}"
            self.agents.append(AgentState(
                id=f"Agent_{i}",
                species=species,
                x=random.randint(0, self.world_size['x_dim'] - 1),
                y=random.randint(0, self.world_size['y_dim'] - 1),
                energy=100.0,
                well_being=0.7, # Starting well-being
                cooperation_tendency=base_coop,
                is_alive=True
            ))
        
        # Apply agent_behavior rules to initial agents
        for rule_entry in self.generated_rules.get('agent_behaviors', []):
            if "Agent_Cooperation" in rule_entry['id']:
                coop_factor = rule_entry['parameters'].get('cooperation_reward_factor', 1.0)
                for agent in self.agents:
                    agent.cooperation_tendency = np.clip(agent.cooperation_tendency * coop_factor, 0.0, 1.0)
        
        logger.debug(f"Initialized {len(self.agents)} agents.")

        # Initialize Resources
        num_resource_nodes = int(self.world_size['x_dim'] * self.world_size['y_dim'] / 25) # Roughly 1 node per 5x5 area
        base_regen_rate = self.sim_settings.get('base_resource_regen_rate', 0.01)

        for i in range(num_resource_nodes):
            self.resources.append(ResourceNode(
                x=random.randint(0, self.world_size['x_dim'] - 1),
                y=random.randint(0, self.world_size['y_dim'] - 1),
                type="BasicResource",
                amount=random.uniform(50.0, 150.0),
                regen_rate=base_regen_rate
            ))
        
        # Apply environmental_law rules to initial resources
        for rule_entry in self.generated_rules.get('environmental_laws', []):
            if "Resource_Regen" in rule_entry['id']:
                regen_multiplier = rule_entry['parameters'].get('regen_health_multiplier', 1.0)
                for res in self.resources:
                    res.regen_rate = np.clip(res.regen_rate * regen_multiplier, 0.0, 0.1) # Max regen 10%

        logger.debug(f"Initialized {len(self.resources)} resource nodes.")

    def _apply_rules_and_step(self, sim_state: SimulationState) -> SimulationState:
        """
        Applies generated rules for a single time step and updates the simulation state.
        This is a highly simplified interpretation of complex AI-designed rules.
        """
        new_agents = [agent for agent in sim_state.agents if agent.is_alive]
        new_resources = list(sim_state.resources) # Make a copy to modify

        current_log_messages = []

        # --- Agent Behaviors ---
        for agent in new_agents:
            if not agent.is_alive: continue

            # Movement (simple random walk)
            agent.x = np.clip(agent.x + random.choice([-1, 0, 1]), 0, self.world_size['x_dim'] - 1)
            agent.y = np.clip(agent.y + random.choice([-1, 0, 1]), 0, self.world_size['y_dim'] - 1)
            agent.energy -= self.sim_settings.get('base_agent_energy_consumption', 0.05)

            # Resource Interaction
            nearby_resources = [res for res in new_resources if res.x == agent.x and res.y == agent.y]
            if nearby_resources:
                resource_to_consume = random.choice(nearby_resources)
                consumed_amount = min(agent.energy / 5.0, resource_to_consume.amount) # Consume based on need
                agent.energy += consumed_amount * 2 # Gain energy
                resource_to_consume.amount -= consumed_amount
                current_log_messages.append(f"Agent {agent.id} consumed {consumed_amount:.2f} from resource at ({agent.x},{agent.y})")
            
            # Cooperation/Social Dynamics
            nearby_agents = [a for a in new_agents if a.id != agent.id and a.x == agent.x and a.y == agent.y]
            if nearby_agents and random.random() < agent.cooperation_tendency:
                cooperating_agent = random.choice(nearby_agents)
                # Example: Share energy if one is low
                if agent.energy < 50 and cooperating_agent.energy > 80:
                    transfer = min(10.0, cooperating_agent.energy - 80)
                    agent.energy += transfer
                    cooperating_agent.energy -= transfer
                    agent.well_being += 0.01 # Reward cooperation
                    cooperating_agent.well_being += 0.01
                    current_log_messages.append(f"Agent {agent.id} received {transfer:.2f} energy from {cooperating_agent.id} (Cooperation rule).")

            # Check survival
            if agent.energy <= 0:
                agent.is_alive = False
                agent.well_being = 0.0 # Well-being drops to zero upon death
                current_log_messages.append(f"Agent {agent.id} died at step {sim_state.time_step}.")

        # --- Resource Mechanics ---
        for res in new_resources:
            if res.amount < 1000: # Max amount for demonstration
                res.amount += res.regen_rate * res.amount # Exponential regen based on remaining amount
            res.amount = np.clip(res.amount, 0.0, 1000.0) # Clamp max amount

        # --- Update World Metrics ---
        live_agents = [a for a in new_agents if a.is_alive]
        total_energy = sum(a.energy for a in live_agents)
        total_well_being = sum(a.well_being for a in live_agents)
        avg_flourishing = total_well_being / max(1, len(live_agents))
        
        total_resource_amount = sum(res.amount for res in new_resources)

        world_metrics = {
            'live_agent_count': len(live_agents),
            'total_agent_energy': total_energy,
            'avg_agent_flourishing': avg_flourishing,
            'total_resource_amount': total_resource_amount,
            'axioms_active_flags': {
                'environmental_axioms_active': self.sim_settings.get('environmental_axioms_active', False),
                'agent_axioms_active': self.sim_settings.get('agent_axioms_active', False)
            }
        }
        
        # Apply specific rules from generated_rules (simplified example)
        # This is where the complex rules generated by the AI would truly come into play.
        # For instance, a 'Flourishing_Feedback' rule might adjust base_resource_regen_rate
        # based on avg_agent_flourishing.
        for rule_entry in self.generated_rules.get('system_mechanics', []):
            if "Flourishing_Feedback" in rule_entry['id']:
                sensitivity = rule_entry['parameters'].get('resource_adjustment_sensitivity', 0.1)
                if avg_flourishing < 0.5: # If flourishing is low, boost resource regen
                    for res in new_resources:
                        res.regen_rate += sensitivity * 0.01
                    current_log_messages.append(f"System boosted resource regen due to low flourishing (Flourishing_Feedback rule).")
        
        new_sim_state = SimulationState(
            time_step=sim_state.time_step + 1,
            agents=new_agents,
            resources=new_resources,
            world_metrics=world_metrics,
            log_messages=current_log_messages
        )
        return new_sim_state

    def run_simulation(self, output_log_path: str = "data/sim_logs", log_interval: int = 100):
        """
        Runs the simulation from the initial state to max_simulation_steps.

        Args:
            output_log_path (str): Directory to save the simulation log.
            log_interval (int): How often (in steps) to log the full simulation state.
        """
        os.makedirs(output_log_path, exist_ok=True)
        log_filename = os.path.join(output_log_path, f"{self.world_name}_sim_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl")

        current_state = SimulationState(
            time_step=0,
            agents=list(self.agents), # Initial states
            resources=list(self.resources),
            world_metrics={
                'live_agent_count': len(self.agents),
                'total_agent_energy': sum(a.energy for a in self.agents),
                'avg_agent_flourishing': sum(a.well_being for a in self.agents) / max(1, len(self.agents)),
                'total_resource_amount': sum(res.amount for res in self.resources),
                'axioms_active_flags': {
                    'environmental_axioms_active': self.sim_settings.get('environmental_axioms_active', False),
                    'agent_axioms_active': self.sim_settings.get('agent_axioms_active', False)
                }
            }
        )
        self.simulation_log.append(current_state)
        
        logger.info(f"Starting simulation for '{self.world_name}'...")
        with open(log_filename, 'w') as f_log:
            # Log initial state
            f_log.write(json.dumps(self._serialize_state(current_state)) + '\n')

            for step in range(1, self.max_simulation_steps + 1):
                if current_state.world_metrics.get('live_agent_count', 0) == 0:
                    logger.warning(f"All agents died at step {step-1}. Ending simulation prematurely.")
                    break
                
                current_state = self._apply_rules_and_step(current_state)
                
                if step % log_interval == 0 or step == self.max_simulation_steps:
                    self.simulation_log.append(current_state)
                    f_log.write(json.dumps(self._serialize_state(current_state)) + '\n')
                    logger.debug(f"Step {step}: Agents={current_state.world_metrics['live_agent_count']}, Flourishing={current_state.world_metrics['avg_agent_flourishing']:.2f}")

        logger.success(f"Simulation for '{self.world_name}' completed. Log saved to: {log_filename}")
        return log_filename

    def _serialize_state(self, state: SimulationState) -> Dict[str, Any]:
        """Converts a SimulationState object into a JSON-serializable dictionary."""
        return {
            'time_step': state.time_step,
            'agents': [agent.__dict__ for agent in state.agents],
            'resources': [res.__dict__ for res in state.resources],
            'world_metrics': state.world_metrics,
            'log_messages': state.log_messages
        }

# --- Example Usage (for testing and demonstration) ---
if __name__ == "__main__":
    # Ensure src/utils directory and logger.py exist for setup_logging
    if not os.path.exists("src/utils"):
        os.makedirs("src/utils")
        # Assuming logger.py is already there or will be created next
    
    # Create dummy output directory if it doesn't exist
    if not os.path.exists("data/sim_logs"):
        os.makedirs("data/sim_logs")

    logger.info("--- Demonstrating TemplateSimulator ---")

    # Simulate a compiled_world_config (as if from world_compiler.py)
    # This dummy data should mimic the structure produced by world_compiler.py
    dummy_compiled_world_config = {
        'simulation_defaults': {
            'simulation_engine_version': "generic_agent_based_v1.0",
            'initial_world_size': {'x_dim': 50, 'y_dim': 50, 'z_dim': 1},
            'time_step_duration_ms': 50,
            'max_simulation_steps': 500, # Shorter for demo
            'initial_agent_count': 30,
            'agent_species_diversity': 3,
            'base_agent_energy_consumption': 0.1,
            'base_resource_regen_rate': 0.005,
            'initial_agent_cooperation_tendency': 0.6,
            'environmental_diversity_index': 0.8,
            'environmental_axioms_active': True,
            'agent_axioms_active': True
        },
        'world_metadata': {
            'name': "MySimulatedWorld",
            'creation_timestamp': datetime.datetime.now().isoformat(),
            'axioms_influencing_design': ['PHILOSOPHY_FLOURISHING_001', 'ECOLOGY_SUSTAINABILITY_001'],
            'designed_by': "Ontological Playground Designer AI",
            'generation_meta': {}
        },
        'generated_world_rules': {
            'agent_behaviors': [
                {'id': 'Agent_Cooperation_Rule_001', 'description': 'Agents gain well-being from cooperative actions.', 'type': 'agent_behavior', 'parameters': {'cooperation_reward_factor': 1.2}, 'dependencies': []}
            ],
            'environmental_laws': [
                {'id': 'Resource_Regen_Rule_001', 'description': 'Resource regeneration rate tied to ecological health.', 'type': 'environmental_law', 'parameters': {'regen_health_multiplier': 1.5}, 'dependencies': []}
            ],
            'system_mechanics': [
                {'id': 'Flourishing_Feedback_Rule_001', 'description': 'System boosts resource regen if avg flourishing is low.', 'type': 'system_mechanic', 'parameters': {'resource_adjustment_sensitivity': 0.05}, 'dependencies': []}
            ]
        }
        # rule_interdependencies_graph would be here in a real config
    }

    # 1. Initialize simulator
    simulator = TemplateSimulator(dummy_compiled_world_config)

    # 2. Run simulation
    log_file = simulator.run_simulation()

    logger.info(f"\n[bold green]Simulation for '{simulator.world_name}' completed. Log saved to: {log_file}[/bold green]")
    logger.info("This log file can now be used for evaluation and visualization.")
